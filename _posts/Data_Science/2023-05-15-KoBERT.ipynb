{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**5월 12일 기준, 코랩에서 작동 가능한 코드**\n",
        "\n",
        "- 코랩에서 파이썬 버전을 업데이트 하면서 버그가 발생했을 것이라 추정.\n",
        "- [깃헙 이슈 페이지](https://github.com/SKTBrain/KoBERT/issues)에서 고수님들의 대화를 통해 버그를 해결할 수 있었음"
      ],
      "metadata": {
        "id": "RX8FjHJLYBRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- SK TBrain에서 공개한 BERT모델\n",
        "- 한국어에 특화된 모델이며, 이 모델을 통해 예제 리뷰를 예측 및 정확도 평가를 측정하여 모델의 신뢰성을 확인\n",
        "- 본 태스크는 네이버 영화 리뷰 데이터를 감성분석하는 목적으로 **이진분류**를 진행하여 모델의 정확도를 측정\n"
      ],
      "metadata": {
        "id": "x58FIfH6XNUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 & 모델 다운로드\n",
        "\n",
        "- 네이버 영화 리뷰 데이터 (NSMC)\n",
        "- KoBERT 패키지\n",
        "- transformer 설치\n",
        "\n",
        "필요한 라이브러리 설치"
      ],
      "metadata": {
        "id": "bURHxnQqXgwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mxnet\n",
        "!pip install gluonnlp==0.8.0\n",
        "!pip install tqdm pandas\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "\n",
        "# kobert\n",
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n",
        "# NSMC\n",
        "!git clone https://github.com/e9t/nsmc.git"
      ],
      "metadata": {
        "id": "q55kx9PHXNMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라이브러리"
      ],
      "metadata": {
        "id": "E0dYVuTMYU7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "# 자칫하면 오류날 수도 있는 부분\n",
        "import gluonnlp as nlp\n",
        "\n",
        "# ★ Hugging Face를 통한 모델 및 토크나이저 Import\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ],
      "metadata": {
        "id": "ie9vV-6wXNKY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11a67978-2911-4ff2-bc46-85d4f449d16b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/mxnet/optimizer/optimizer.py:163: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  warnings.warn('WARNING: New optimizer %s.%s is overriding '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 2021\n",
        "deterministic = True\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "UQu6aZX92KUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 읽기\n",
        "train = pd.read_csv(\"./nsmc/ratings_train.txt\", sep='\\t')\n",
        "test = pd.read_csv(\"./nsmc/ratings_test.txt\", sep='\\t')\n",
        "\n",
        "# 데이터 줄이기 (빠르게 학습 결과를 보기 위함)\n",
        "# train = train.iloc[:20000, :]\n",
        "# test = test.iloc[:5000, :]\n",
        "\n",
        "# 결측치 제거\n",
        "train = train.dropna()\n",
        "test = test.dropna()\n",
        "print('train shape:',train.shape)\n",
        "print('test shape:',test.shape)\n",
        "print()\n",
        "\n",
        "# GPU 확인 및 사용\n",
        "n_devices = torch.cuda.device_count()\n",
        "print('device count:', n_devices)\n",
        "print()\n",
        "\n",
        "# GPU 있으면 할당\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#  ★ Hugging Face을 통한 BERT 모델, Vocabulary 불러오기\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
        "\n",
        "print(bertmodel)\n",
        "print('-'*100)\n",
        "print(vocab)\n",
        "print(\"Current device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sY3VJo-XNIp",
        "outputId": "359cdf09-124e-4052-acab-4c49ed1808f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
            "The class this function is called from is 'KoBERTTokenizer'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train shape: (149995, 3)\n",
            "test shape: (49997, 3)\n",
            "\n",
            "device count: 1\n",
            "\n",
            "BertModel(\n",
            "  (embeddings): BertEmbeddings(\n",
            "    (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
            "    (position_embeddings): Embedding(512, 768)\n",
            "    (token_type_embeddings): Embedding(2, 768)\n",
            "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (encoder): BertEncoder(\n",
            "    (layer): ModuleList(\n",
            "      (0-11): 12 x BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pooler): BertPooler(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (activation): Tanh()\n",
            "  )\n",
            ")\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Vocab(size=8002, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n",
            "Current device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(train[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "RkWvMHT3XNHP",
        "outputId": "d1fc9795-1449-4710-9988-fdec3dd49faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         id                                           document  label\n",
              "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
              "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
              "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9cee2b37-fe93-4608-8f5c-06c8d4590cc3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6483659</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9cee2b37-fe93-4608-8f5c-06c8d4590cc3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9cee2b37-fe93-4608-8f5c-06c8d4590cc3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9cee2b37-fe93-4608-8f5c-06c8d4590cc3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 전처리\n",
        "\n",
        "입력 데이터를 만들기 위해 데이터프레임으로 되어 있던 타입을 아래와 같이 리스트 형태로 변환한다.\\\n",
        "[[document(1), label(1)], [document(2), label(2)], ..., [document(n), label(n)]]"
      ],
      "metadata": {
        "id": "8a0tHSuf7MUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame -> List\n",
        "\n",
        "dataset_train = []\n",
        "for docu, label in zip(train.document, train.label):\n",
        "  if '\"' in docu:\n",
        "    docu = re.sub('\"', '', docu)\n",
        "  dataset_train.append([docu, str(label)])\n",
        "\n",
        "dataset_test = []\n",
        "for docu, label in zip(test.document, test.label):\n",
        "  if '\"' in docu:\n",
        "    docu = re.sub('\"', '', docu)\n",
        "  dataset_test.append([docu, str(label)])\n",
        "\n",
        "\n",
        "print(dataset_train[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAl54o17XNFi",
        "outputId": "88abd794-9459-4d59-ad77-bdbb9591154e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['아 더빙.. 진짜 짜증나네요 목소리', '0'], ['흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', '1'], ['너무재밓었다그래서보는것을추천한다', '0']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "KoBERT의 입력 데이터로 만들기 위해서 다음과 같이 전처리 작업을 추가적으로 진행한다.\n",
        "\n",
        "- 토큰화\n",
        "- 정수 인코딩\n",
        "- 패딩"
      ],
      "metadata": {
        "id": "zUlv8OmT8GEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTSentenceTransform:\n",
        "    def __init__(self, tokenizer, max_seq_length,vocab, pad=True, pair=True):\n",
        "        self._tokenizer = tokenizer\n",
        "        self._max_seq_length = max_seq_length\n",
        "        self._pad = pad\n",
        "        self._pair = pair\n",
        "        self._vocab = vocab \n",
        "\n",
        "    def __call__(self, line):\n",
        "        # convert to unicode\n",
        "        text_a = line[0]\n",
        "        if self._pair:\n",
        "            assert len(line) == 2\n",
        "            text_b = line[1]\n",
        "\n",
        "        ##### 여기 수정!! #####\n",
        "        tokens_a = self._tokenizer.tokenize(text_a)\n",
        "        tokens_b = None\n",
        "\n",
        "        if self._pair:\n",
        "            tokens_b = self._tokenizer(text_b)\n",
        "\n",
        "        if tokens_b:\n",
        "            self._truncate_seq_pair(tokens_a, tokens_b,\n",
        "                                    self._max_seq_length - 3)\n",
        "        else:\n",
        "            if len(tokens_a) > self._max_seq_length - 2:\n",
        "                tokens_a = tokens_a[0:(self._max_seq_length - 2)]\n",
        "        vocab = self._vocab\n",
        "        tokens = []\n",
        "        tokens.append(vocab.cls_token)\n",
        "        tokens.extend(tokens_a)\n",
        "        tokens.append(vocab.sep_token)\n",
        "        segment_ids = [0] * len(tokens)\n",
        "\n",
        "        if tokens_b:\n",
        "            tokens.extend(tokens_b)\n",
        "            tokens.append(vocab.sep_token)\n",
        "            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))\n",
        "\n",
        "        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The valid length of sentences. Only real  tokens are attended to.\n",
        "        valid_length = len(input_ids)\n",
        "\n",
        "        if self._pad:\n",
        "            # Zero-pad up to the sequence length.\n",
        "            padding_length = self._max_seq_length - valid_length\n",
        "            # use padding tokens for the rest\n",
        "            input_ids.extend([vocab[vocab.padding_token]] * padding_length)\n",
        "            segment_ids.extend([0] * padding_length)\n",
        "\n",
        "        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\\\n",
        "            np.array(segment_ids, dtype='int32')\n",
        "\n",
        "\n",
        "\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len, pad, pair):\n",
        "        transform = BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, vocab=vocab, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ],
      "metadata": {
        "id": "gPENjLkGXND_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "사용자 파라미터 정의를 위한 전처리"
      ],
      "metadata": {
        "id": "cVCXnqGo95_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 가장 긴 문장의 리뷰\n",
        "max_len = 0\n",
        "for i in train.document:\n",
        "  if max_len < len(i):\n",
        "    max_len = len(i)\n",
        "  else:\n",
        "    continue"
      ],
      "metadata": {
        "id": "G_EICniCDid3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위에 정의한 토큰함수를 기반으로 데이터 토큰화\n",
        "\n"
      ],
      "metadata": {
        "id": "LyT32aWRDPFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = BERTDataset(dataset_train, 0, 1, tokenizer, vocab, max_len, True, False) # input:{'문장', '긍부정 라벨'}\n",
        "test = BERTDataset(dataset_test, 0, 1, tokenizer, vocab, max_len, True, False) # input:{'문장', '긍부정 라벨'}"
      ],
      "metadata": {
        "id": "5UO8FFzgXNBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_train의 첫 번재[0] 데이터\n",
        "print(train[0][0]) # 패딩된 시퀀스\n",
        "print(train[0][1]) # 길이\n",
        "print(train[0][2]) # 어텐션 마스크 시퀀스 (함께 입력되어야 함)\n",
        "print(train[0][3]) # 정답 Label\n",
        "print('-'*50)\n",
        "\n",
        "print(train[0]) # pair 형태"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWdGjS2dY30Q",
        "outputId": "bd0b8122-e1e1-41ae-9f39-b6230652625a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   2 3093 1698 6456   54   54 4368 4396 7316 5655 5703 2073    3    1\n",
            "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
            "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
            "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
            "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
            "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
            "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
            "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
            "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
            "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
            "    1    1    1    1    1    1]\n",
            "13\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "0\n",
            "--------------------------------------------------\n",
            "(array([   2, 3093, 1698, 6456,   54,   54, 4368, 4396, 7316, 5655, 5703,\n",
            "       2073,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "          1,    1,    1], dtype=int32), array(13, dtype=int32), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32), 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Torch 입력 데이터 형식으로 변환"
      ],
      "metadata": {
        "id": "lAxpf605ZZay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train, batch_size=batch_size)\n",
        "test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size)\n",
        "\n",
        "print(len(train_dataloader), len(test_dataloader)) # 각 Iteration의 수 (= len(data_train)/batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QV45mtDZY3xo",
        "outputId": "3ba57f8c-feec-4160-9a36-e6cea915df96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2344 782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(train_dataloader):\n",
        "  print(token_ids)\n",
        "  print(valid_length)\n",
        "  print(segment_ids)\n",
        "  break"
      ],
      "metadata": {
        "id": "Mnfkh7sBnDZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9de071fb-f197-469d-937b-2f443c93267a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   2, 3093, 1698,  ...,    1,    1,    1],\n",
            "        [   2,  517, 7989,  ...,    1,    1,    1],\n",
            "        [   2, 1458, 7191,  ...,    1,    1,    1],\n",
            "        ...,\n",
            "        [   2, 2485, 6821,  ...,    1,    1,    1],\n",
            "        [   2, 4841, 6333,  ...,    1,    1,    1],\n",
            "        [   2, 2355, 5842,  ...,    1,    1,    1]], dtype=torch.int32)\n",
            "tensor([13, 25, 14, 22, 46, 33, 15, 67, 13, 36, 16, 32, 29, 31, 34, 13, 45, 23,\n",
            "        32, 24, 26, 10, 81, 16, 11, 35, 19,  9,  6, 27, 30, 23, 12, 15, 13, 13,\n",
            "        23, 19, 14, 12, 11, 52, 17, 27, 55, 17, 77, 38, 13, 97, 32, 30, 17, 37,\n",
            "         4,  5,  3, 65,  7, 11, 14, 19, 24, 56], dtype=torch.int32)\n",
            "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KoBERT\n",
        "\n",
        "num_classes 변수 사용해서 classification의 분류 수에 맞게 설정"
      ],
      "metadata": {
        "id": "0a2ZOZHcalk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KoBERTClassifier(nn.Module):\n",
        "  def __init__(self, bert, hidden_size=768, num_classes=2, dr_rate=None, params=None):\n",
        "    super(KoBERTClassifier, self).__init__()\n",
        "\n",
        "    self.bert = bert\n",
        "    self.dr_rate = dr_rate\n",
        "\n",
        "    self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "    if dr_rate:\n",
        "      self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "  def gen_attention_mask(self, token_ids, valid_length):\n",
        "    attention_mask = torch.zeros_like(token_ids)\n",
        "    for i, v in enumerate(valid_length):\n",
        "      attention_mask[i][:v] = 1\n",
        "    return attention_mask.float()\n",
        "\n",
        "  def forward(self, token_ids, valid_length, segment_ids):\n",
        "    attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "\n",
        "    _, pooler = self.bert(input_ids=token_ids, token_type_ids=segment_ids.long(), attention_mask=attention_mask.float().to(token_ids.device))\n",
        "    if self.dr_rate:\n",
        "      out = self.dropout(pooler)\n",
        "    out = self.classifier(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "3G0rN-zya4X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "사용자 정의 파라미터"
      ],
      "metadata": {
        "id": "zKChLABwo0qX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting parameters\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 2 # test 용도 (램이 딸려서 중간에 끊김..)\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  4e-5"
      ],
      "metadata": {
        "id": "SCAO87H_o2Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pretrained model 불러오고, 최적화, 손실함수, 스케쥴 등등 설정"
      ],
      "metadata": {
        "id": "TNqzZxAAnrJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT 모델 불러오기\n",
        "model = KoBERTClassifier(bertmodel, dr_rate=0.3).to(device)\n",
        "\n",
        "# optimizer와 schedule 설정\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
        "\n",
        "#정확도 측정을 위한 함수 정의\n",
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc"
      ],
      "metadata": {
        "id": "mmTFuMCynRP5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea8d1fd7-96f0-4e6c-a709-c6059108e2ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_acc = 0\n",
        "for e in range(num_epochs):\n",
        "  train_acc = 0.0\n",
        "  test_acc = 0.0\n",
        "\n",
        "  model.train()\n",
        "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
        "    optimizer.zero_grad()\n",
        "    token_ids = token_ids.long().to(device)\n",
        "    segment_ids = segment_ids.long().to(device)\n",
        "\n",
        "    valid_length = valid_length\n",
        "    label = label.long().to(device) # 정답\n",
        "    out = model(token_ids, valid_length, segment_ids) # 예측\n",
        "\n",
        "    loss = loss_fn(out, label)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "    optimizer.step()\n",
        "    scheduler.step() # Update learning rate schedule\n",
        "    train_acc += calc_accuracy(out, label)\n",
        "    if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "  print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "\n",
        "  # 평가\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):\n",
        "      token_ids = token_ids.long().to(device)\n",
        "      segment_ids = segment_ids.long().to(device)\n",
        "      valid_length = valid_length\n",
        "\n",
        "      label = label.long().to(device)\n",
        "      out = model(token_ids, valid_length, segment_ids)\n",
        "\n",
        "      test_acc += calc_accuracy(out, label)\n",
        "    \n",
        "    total_test_acc = test_acc / (batch_id+1)\n",
        "    print(\"epoch {} test acc {}\".format(e+1, total_test_acc))\n",
        "\n",
        "  # 모델 저장\n",
        "  if eval_acc < total_test_acc:\n",
        "    eval_acc = total_test_acc\n",
        "    torch.save(model.state_dict(), f'./model_state_dict.pt')"
      ],
      "metadata": {
        "id": "GLE0Lff1nQ7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e35ec38-bae3-4dde-844a-d87f726aaf91"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/2344 [00:03<2:34:50,  3.97s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 batch id 1 loss 0.7078469395637512 train acc 0.53125\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▊         | 201/2344 [04:37<49:26,  1.38s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 batch id 201 loss 0.39718449115753174 train acc 0.644589552238806\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 401/2344 [09:15<45:05,  1.39s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 batch id 401 loss 0.36952823400497437 train acc 0.7321150249376559\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 601/2344 [13:55<40:54,  1.41s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 batch id 601 loss 0.4212072789669037 train acc 0.7703826955074875\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|███▍      | 801/2344 [18:35<36:24,  1.42s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 batch id 801 loss 0.4127542972564697 train acc 0.7919202559300874\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 1001/2344 [23:13<30:57,  1.38s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 batch id 1001 loss 0.3550129234790802 train acc 0.8056006493506493\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 51%|█████     | 1201/2344 [27:50<26:22,  1.38s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 batch id 1201 loss 0.31423938274383545 train acc 0.8160777477102414\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|█████▉    | 1401/2344 [32:27<21:46,  1.39s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 batch id 1401 loss 0.4369466006755829 train acc 0.8232624018558172\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 68%|██████▊   | 1601/2344 [37:04<17:06,  1.38s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 batch id 1601 loss 0.2754068970680237 train acc 0.8300183479075578\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 77%|███████▋  | 1801/2344 [41:41<12:33,  1.39s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 batch id 1801 loss 0.25950542092323303 train acc 0.8355167268184343\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 85%|████████▌ | 2001/2344 [46:17<07:54,  1.38s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 batch id 2001 loss 0.3744203448295593 train acc 0.8403142178910544\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 94%|█████████▍| 2201/2344 [50:54<03:17,  1.38s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 batch id 2201 loss 0.27376240491867065 train acc 0.844573489323035\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2344/2344 [54:11<00:00,  1.39s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 train acc 0.8473601575521867\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [06:15<00:00,  2.08it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 test acc 0.8929443119220932\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/2344 [00:01<54:43,  1.40s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2 batch id 1 loss 0.48298001289367676 train acc 0.796875\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▊         | 201/2344 [04:38<49:29,  1.39s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2 batch id 201 loss 0.18480078876018524 train acc 0.8889148009950248\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 401/2344 [09:15<44:52,  1.39s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2 batch id 401 loss 0.23000317811965942 train acc 0.8945994389027432\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 601/2344 [13:51<39:59,  1.38s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2 batch id 601 loss 0.324882835149765 train acc 0.9013103161397671\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|███▍      | 801/2344 [18:28<35:36,  1.38s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2 batch id 801 loss 0.2671748399734497 train acc 0.9058793695380774\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 1001/2344 [23:05<30:55,  1.38s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2 batch id 1001 loss 0.2897033989429474 train acc 0.9084040959040959\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 51%|█████     | 1201/2344 [27:41<26:19,  1.38s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2 batch id 1201 loss 0.12562845647335052 train acc 0.9112458368026645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|█████▉    | 1401/2344 [32:18<21:43,  1.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 batch id 1401 loss 0.14803026616573334 train acc 0.9137892576730906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 68%|██████▊   | 1601/2344 [36:55<17:09,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 batch id 1601 loss 0.222313791513443 train acc 0.9152287632729544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 77%|███████▋  | 1801/2344 [41:32<12:31,  1.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 batch id 1801 loss 0.12680453062057495 train acc 0.9166782343142699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|████████▌ | 2001/2344 [46:09<07:55,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 batch id 2001 loss 0.28915905952453613 train acc 0.9180722138930535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 94%|█████████▍| 2201/2344 [50:46<03:18,  1.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 batch id 2201 loss 0.15833142399787903 train acc 0.9191773625624716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2344/2344 [54:03<00:00,  1.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 train acc 0.9200489932236685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [06:16<00:00,  2.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 test acc 0.9000359654731458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "lpkBCp9IN76v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 테스트 데이터셋 예측"
      ],
      "metadata": {
        "id": "PdoRSdQepcMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, sentence):\n",
        "  global tokenizer, vocab, max_len, device\n",
        "\n",
        "  data = [[sentence, '0']] # 입력 포맷\n",
        "  \n",
        "  tokenized_data = BERTDataset(data, 0, 1, tokenizer, vocab, max_len, True, False)\n",
        "  tensor_data = torch.utils.data.DataLoader(tokenized_data)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    token_ids, valid_len, segment_ids, label = next(iter(tensor_data))\n",
        "    \n",
        "    token_ids = token_ids.long().to(device)\n",
        "    segment_ids = segment_ids.long().to(device)\n",
        "    \n",
        "    out = model(token_ids, valid_len, segment_ids)\n",
        "    softmax_out = nn.functional.softmax(out, dim=1)\n",
        "\n",
        "  return softmax_out"
      ],
      "metadata": {
        "id": "hbRoIxv-5Szv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv(\"./nsmc/ratings_test.txt\", sep='\\t')"
      ],
      "metadata": {
        "id": "iTK7mloWWH1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in test.document:\n",
        "  sentence = '애매 하지만 낫밷'\n",
        "  softmax_out = predict(model, sentence)\n",
        "  pred_label = softmax_out.argmax(dim=1)\n",
        "  break"
      ],
      "metadata": {
        "id": "J1jk5bWmYEWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax_out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sgSejitYokW",
        "outputId": "32762927-2535-4506-ce8c-1fa87dface29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0669, 0.9331]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zA8935CmYOcj",
        "outputId": "7b4d5da8-0dc1-4089-aebc-a40dda5cb0d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    }
  ]
}