---
layout: single
title: "[Text Representation-1] 단어 표현과 단어 임베딩 개념"
categories:	
    - nlp
tags:
    - ['딥러닝', '알고리즘', '자연어처리']

toc: true
toc_sticky : true

author_profile: false
sidebar:
    nav: "docs"

last_modified_at:
use_math: true
---

# 단어 임베딩(Word Embedding)

- 텍스트를 컴퓨터가 이해하고, 효율적으로 처리하기 위해 컴퓨터가 이해할 수 있도록 **텍스트를 적절히 숫자로 변환하는 것이 목적**
- 현재에 이르러 각 단어를 인공 신경망 학습을 통해 벡터화하는 워드 임베딩이라는 방법이 가장 많이 사용되고 있음

---

<br/>

# 희소 표현 (Sparse Representation)

- 원-핫 인코딩을 통해 나온 원-핫 벡터들을 표현하고자 하는 단어의 **인덱스 값**만 **1이고, 나머지 인덱스 값은 전부 0**으로 표현하는 것처럼 **벡터 또는 행렬**의 값이 대부분 0으로 표현되는 방법
- **단어의 크기가 벡터의 차원을 결정함**
- 원-핫 벡터는 희소 벡터(sparse vector)라고 함

### 희소 벡터의 문제점

- 단어의 개수가 늘어나면 벡터의 차원이 없이 커짐
  - 예를 들어, Corpus에 단어가 10,000개였다면 벡터의 차원은 10,000이 되버림
- 단어의 인덱스에 해당되는 부분만 1이고 나머지는 0인 값을 갖게 되면서 <u>공간정 낭비가 발생</u>
  - 예를 들어, 뚱이 = [ 0 0 0 0 1 0 0 0 0 0 0 0 ... 중략 ... 0]  -> 이때 1 뒤의 0의 수는 9995개

<br/>

# 밀집 표현 (Dense Representation)

- 희소 표현과 반대되는 개념으로써 **백터의 차원을 단어 집합의 크기로 정하지 않음**
- 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞추며 이때, **실수 값으로 표현**
  - 예를 들어, 스폰지밥 = [0.2 1.8 1.1 -2.1 1.1 2.8 ... 중략 ...] # 이 벡터의 차원은 128

- 이 경우 벡터 차원이 조밀해졌다 해서 밀집 벡터(dense vector)라고 표현

<br/>

# 단어 임베딩 (Word Embedding)

- 단어를 <u>밀집 벡터(dense vector)</u>의 형태로 표현하는 방법을 **워드 임베딩(word embedding)**
- 워드 임베딩 방법론
  - LSA
  - WordVec (2014)
  - Glove (2014)
  - FastText (2017)
- 단어를 랜덤한 값을 가지는 밀집 벡터로 변환한 뒤, 인공 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습하는 방법을 사용



<br/>

<br/>

# 참고자료

---

- [고려대학교 강필성 교수 - 비정형데이터분석 강의](https://www.youtube.com/watch?v=UInnl60pzkA&list=PLetSlH8YjIfVzHuSXtG4jAC2zbEAErXWm)
- [딥러닝을 이용한 자연어처리 입문 - 워드임베딩](https://wikidocs.net/22644)

