---
layout: single
title: "[Text Representation-2] 단어 임베딩과 NNLM"
categories:	
    - nlp
tags:
    - ['딥러닝', '알고리즘', '자연어처리']

toc: true
toc_sticky : true

author_profile: false
sidebar:
    nav: "docs"

last_modified_at:
use_math: true
---

본 포스팅은 고려대학교 강필성 교수님의 [비정형데이터분석 강의](https://www.youtube.com/watch?v=UInnl60pzkA&list=PLetSlH8YjIfVzHuSXtG4jAC2zbEAErXWm)를 보고 정리한 것임을 미리 밝힙니다.

---

**Distributed Representation Models**

> - <span style='background-color: #dcffe4'>NNLM(2003) - Distributed Representation의 개념을 처음 도입한 Word Embedding계의 시조새</span>
> - Word2Vec(2013) - 주변 단어들의 정보를 활용해서 원하는 단어의 Embedding을 학습 (CBOW vs Skip-gram)
> - Glove(2014) - 단어의 동시발생빈도를 고려해서 Embedding을 학습
> - Fasttext(2017) - 학습 과정에서 보지 못한 단어(OOV)들과 형태소 변화에 대응할 수 있도록 Sub-word 개념을 도입

<br/>

# 단어 임베딩 (Word Embedding)

단어 임베딩의 목적은 의미적으로 <u>비슷한 단어들은 벡터공간에서 가깝게 위치</u>할 수 있도록 하는 것이다. 영어의 경우 13여만개의 단어가 존재하나 이보다 <u>훨씬 적은 d차원</u>에 모든 단어를 임베딩 시킬 수 있을 것으로 가정한다.



<br/>

## Word vectors: one-hot vector (원-핫 벡터, <u>Discrete</u>)

- 가장 단순하면서도 직관적인 표현 방식
- 벡터 차원이 단어 집합의 크기
- 벡터 공간에 단어를 표현할 수 있으나, 의미상 <u>유사성을 보존할 수 없음</u>
  - 어떤 두 단어 사이의 <u>내적은 0</u>이 된다


![스크린샷 2023-03-30 오전 11.01.57]({{site.url}}/images/2023-03-29-nlp3/스크린샷 2023-03-30 오전 11.01.57.png){: width="50%" height="50%"}{: .align-center}

$$(w^a)^Tw^{at} = w^{at}(w^a)^T = 0$$

## Word vectors: distributed representation (분산 표현, <u>Continuous</u>)

- 단어들을 특정 공간에 **실수**값을 갖는 벡터로 매핑시키는 함수
- 벡터 차원이 단어 집합의 크기일 필요가 없음
- <u>단어 사이의 의미적 관계가 벡터 공간에서 보존된다</u>

![image-20230330111222112]({{site.url}}/images/2023-03-29-nlp3/image-20230330111222112.png){: width="50%" height="50%"}{: .align-center}

![image-20230330111337753]({{site.url}}/images/2023-03-29-nlp3/image-20230330111337753.png)

- 예시) "King"이란 단어를 50차원으로 학습하는 임베딩 모델을 통해 학습시킨 결과

![image-20230331233745197]({{site.url}}/images/2023-03-29-nlp3/image-20230331233745197.png)

<br/>

희소 표현은 각 차원이 분리된 표현 방법이었다면, 분산 표현은 저차원에 <u>단어의 의미를 여러 차원에 분산</u>하여 표현한다. 이런 표현 방법을 사용하면 **단어 벡터 간 유의미한 유사도**를 계산할 수 있다.

<br/>

---

# Neural Network Language Model (NNLM)

- 차원의 저주를 무찌르겠다! 라는 목표로부터 시작됨
- Distributed Representation의 개념을 초기에 도입
- 단어 간 벡터를 통해 유사성을 고려함으로써 <u>희소 문제(sparsity problem)</u>을 해결하고자 함
  - 단어들의 조합된 확률을 계산할 수 있음
  - 숫자형 벡터로 만들 때 어떻게 하면 좋은 것인가
  - 일련의 단어들의 시퀀스의 확률을 높게 만들자
- 간단히 말하면 컨셉은 다음과 같음
  - 만약 아래의 예문처럼 '개'와 '고양이'가 비슷한 역할이라면? 다른 것들도 유사성을 통해 일반화 할 수 있지 않을까?
    - <u>The cat is walking in the bedroom</u>
      - A dog was running in a room
      - The cat is running is a room
      - A dog is walking in a bedroom
      - The dog was waling in the room
      - ...

여기서 Count-based Language Model과의 비교를 잠시 살펴본다. 

- 기본적으로 이전에 해당하는 조합들이 주어져 있다고 가정하며 이는 Chain rule을 적용하여 결합확률 분포로 표현이 가능
- 첫 번재 단어 $w_1$부터 $w_T$까지의 결합 확률 분포는 다음과 같이 정의
  - 여기서 결합 분포란 "확률 변수가 여러 개일 때 이들을 함께 고려하는 확률 분포"을 의미
- <u>첫 번째</u> 단어를 보고 다음 단어가 나올 확률 x <u>첫 번째, 두 번째</u> 단어를 보고 다음 단어가 나올 확률  x ... x  <u>첫 번째 , ..., t-1번째</u> 단어를 보고 내가 원하는 단어가 나올 확률과 같음

$$P(w_1, ..., w_T ) = \prod_{t=1}^T P(w_t | w_1, ..., w_{t-1})$$

- i love you의 예시에서,  P(i, love ,you)는 P(love l i) x P(you l love, i)로 표현됨 ((i, love, you)가 동시에 발생(하나의 시퀀스)할 확률)

<br/>

근데 여기서 100개의 단어로 구성된 한 문장의 경우, 99개의 단어를 보고 다음 단어를 예측해야 한다. 이 때 정확하게 99번째까지 단어가 같은 시퀀스 조합을 찾는 것은 거의 불가능하기 때문에 우리는 여기서 Markov assumption을 적용한다. 이는 과거를 다 보는 것이 아닌 몇 개에 대해서만 보자는 의미로 일부 단어만을 보고 다음 단어를 예측하자는 것이다. 그렇다면 여기서 몇 개의 단어를 볼 것인가? 에 대한 물음이 있을 수 있다. 이것이 바로 n-grams 이다. (= 윈도우 안에서 몇 번째 단어까지 볼 것인가)

- 몇 개의 단어를 볼 것인가? (n-grams)
  - uni-grams : 단어 단위로 만들어내자
  - bi-grams : 바로 이전 단어만 보고 예측하자
  - tri-grams : 이전 두 단어를 보고 예측하자

$$P(w_t|w_1, ..., w_t) \approx P(w_t|w_{t-n}, ..., w_{t-1})$$

- 예시
  - bi-grams : 바로 이전 단어(1개)만 보고 다음을 보자
    - I love you에서 love만 보고 you를 예측
  
  - 2-grams : 두 개의 단어만 가지고 다음을 보자
    -  I love you 에서 I, love 만 보고 you를 예측
  

<br/>

## 수학적 이해

- 단어는 더이상 Sparse vector가 아닌, **Dense vector**로 표현됨
  - $w_t \in \mathbb{R^{\left\vert V \right\vert}}$
    - $w_t$: t시점에서 $\left\vert V \right\vert$에 포함된 원-핫 벡터로 표현된 단어
      - t=3 이라면, $w_t $ = [0, 0, 1, 0, 0, ..., 0]으로 표현
      -  $\left\vert V \right\vert$ : Vocabulary 사이즈 만큼의 차원
    
  - $x_t = Xw_t  (X \in \mathbb{R^{n \times \left\vert V \right\vert}},  n < \left\vert V \right\vert)$
    - $X$ : $w_t$을 받아 학습시킬 파라미터이며 $w_t$이 원-핫 벡터이기 때문에 실제 연산은 $t$시점의 행 or 열만 추출됨. 이를 Look up table이라고 부름
      - $X$는 $n \times \left\vert V \right\vert$의 차원을 갖으며 여기서 $n$은 $\left\vert V \right\vert$보다 작아야함 (왜냐하면 $\left\vert V \right\vert$보다 사이즈를 작게 하자는게 목적이었으니까)
    
    - $x_t$: t시점의 단어가 $n$ 차원에 표현된 임베딩

<br/>

여기서 NNLM은 시퀀스를 그대로 넣어서 다음 단어를 예측하는 Neural network를 학습하고자 한다. 예를 들어, 나는 너를 __ 이란 문장에서 '나는', '너를' 이란 단어를 사용하여 __에 들어갈 단어를 예측하는 것이다. 다음 단어가 예측될 확률을 수식적으로 표현한 것은 아래와 같다.

![image-20230401130339247]({{site.url}}/images/2023-03-29-nlp3/image-20230401130339247.png){: width="70%" height="70%"}{: .align-center}

- <u> t-1개의 단어가 주어졌을 때, **t번째 단어가 j일지를 예측**하고자 하는 것</u>
  - $P^j \in \mathbb{R^m}$ : 모델을 통해서 나오는 파라미터 값  (<span style='color: blue'>학습대상</span>)
  - $g$ : $X_1, ..., X_{t-1}$ 을 연산하는 Composition function (<span style='color: blue'>학습대상</span>)
  - $q^j \in \mathbb{R}$ : j단어에 대한 편향 벡터

다시 한번 말해서, 단어 시퀀스 $w_1, ..., w_{t-1}$을 넣으면 (t가 몇일지는 n-grams의 값에 따라 다르다) 신경망을 통해 t번째 단어가 j일 확률을 얻고자 하는 것이다. (j일 확률이란 것은 t자리에 들어갈 단어가 j라는 것을 알고 있다는 것이고, 우리는 이것을 지도학습이라고 부르며, t번째 자리에 j단어로 예측을 잘하는 모델을 만들고 싶은 것이다.)

즉, NNLM의 목적은 전체 시퀀스가 아닌 정해놓은 n개의 순차적인 시퀀스에 한해서 1부터 t-1까지의 단어를 주고, t번째 정답 단어가 나올 확률을 극대화 하는 모델 $f$를 찾는 것이다! 이것을 수식으로 쓰면 아래와 같다.

$$f(w_t, ..., w_{t-n+1}) = \hat{P}(w_t | w_{1}^{t-1})$$

이때, f의 조건은 다음 두 가지를 만족한다.

1. $f \ge 0$
   - f의 결과는 문장 시퀀스의 대한 확률값이기 때문에 항상 0 이상이다.
2. $\sum_{i=1}^{\left\vert V \right\vert} f(t, w_{t-1}, ..., w_{t-n+1})$
   - 모든 vocabulary의 단어 words의 f값의 합은 1이어야 한다.

<br/>

## 신경망 구조

아래의 그림을 통해 신경망의 구조를 살펴보도록 하자.

![]({{site.url}}/images/2023-03-29-nlp3/image-20230401221527033.png){: width="80%" height="80%"}{: .align-center}

- 먼저 Input layer에서 $\left\vert V \right\vert$의 크기 만큼의 원-핫 벡터로 된  n개의 단어 시퀀스가 입력으로 들어간다. 이 때 들어간 원-핫 벡터와 $\left\vert V \right\vert \times M$의 크기를 가진 Look-up table과 연산되면서 i번째 단어는 Look-up table에서의 i번째 행 or 열만을 가져오게 된다.
  - 다른 단어에 대해서도 우리는 Look-up table과 연산되어 Look-up table에서 꺼내오는 형식이기 때문에 "Look-up table을 공유한다" 라고 표현할 수 있다.

![image-20230401222907235]({{site.url}}/images/2023-03-29-nlp3/image-20230401222907235.png){: width="80%" height="80"}{: .align-center}

- 단어 시퀀스에 해당하는 임베딩 값을 모두 뽑았으면 그대로 Concatenate한다. 이 때 다시 한번 더 말하면 Look-up table은 공유되기 때문에 우리는 모델 입력에 대해 임베딩 값들을 얻게 된 것이다.
  - 예를 들면(값은 임의로 정해봄), I love __일 때 I의 임베딩 값은 [1, 0, 0] -> [4, 19, 2] 이고, love의 임베딩 값은 [0, 1, 0] -> [5, 23, 12] 라면, concatenate하면 [4, 19, 2, 5, 23, 12]가 되는 것이다.
- 이후 Hidden layer에 보내져 연산하고, 마지막 layer에서 Softmax 활성화 함수를 통해 각 단어에 대한 확률 값을 결과로 내놓게 된다.
- 위의 방식은 다 실선에 해당되는 얘기인데, 점선으로 표시된 것은 Skip-connection이란 것이다. 이것은 Look-up table에서 가져온 각 단어별 임베딩 값을 concatenate하는 부분에 같이 하자는 것이고 선택사항이 되겠다.

<br/>

**Objective function**은 다음과 같다.

![image-20230401223411641]({{site.url}}/images/2023-03-29-nlp3/image-20230401223411641.png ){: width="50%" height="50%"}{: .align-center}

일반적인 MLE와 같이 neural network의 output log-likelihood를 최대화 한다. $R(\theta)$은 weight decay regularization term이다.

**optimization**은 Stochastic gradient ascent를 사용한다.

![image-20230401223653298]({{site.url}}/images/2023-03-29-nlp3/image-20230401223653298.png){: width="50%" height="50%"}{: .align-center}

일련의 학습과정을 통해 distributed word representation table C가 완성되면 우리는 단어의 임베딩 벡터값을 얻을 수 있게 된다.

<br/>

## 결과

실험은 Perplexity를 사용하여 모델을 비교한다.

![image-20230401223911201]({{site.url}}/images/2023-03-29-nlp3/image-20230401223911201.png){: width="50%" height="50%"}{: .align-center}

Perplexity는 식과 같이 주어진 문장에서 $\hat{P}(w_t \| w_1^{t-1})$의 역수인 $\frac{1}{\hat{P}(w_t \| w_1^{t-1})}$의 기하평균이라고 생각하면 된다. 이는 문장의 발생 확률이 높을수록 각각의 $\frac{1}{\hat{P}(w_t \| w_1^{t-1})}$값은 낮아지기 때문에, Perplexity가 낮다는 것은 주어진 문장의 발생 확률이 높아 언어 모델이 잘 학습되었음을 의미한다.

![image-20230401225523198]({{site.url}}/images/2023-03-29-nlp3/image-20230401225523198.png){: width="70%" height="70%"}{: .align-center}

실험 결과 MLP(neural network)를 사용한 모델이 n-gram based의 다른 모델과 비교했을 때, 가장 낮은 test perplexity를 보이며 보다 양질의 언어모델을 학습하였음을 확인할 수 있다. Skip-connection이 유용한지에 대해선 확실한 답은 내릴 수 없지만, 적은 데이터셋에 대해선 분명한 generalization 효과를 보였고 약간의 perplexity가 증가 현상이 있어 손해가 있지만 학습시간이 꽤 많이 단축되었다고 한다.



<br/>

<br/>

# 참고자료

---

- [고려대학교 강필성 교수 - 비정형데이터분석 강의](https://www.youtube.com/watch?v=bvSHJG-Fz3Y&list=PLetSlH8YjIfVzHuSXtG4jAC2zbEAErXWm&index=7)
- [딥러닝을 이용한 자연어처리 입문 - 워드임베딩](https://wikidocs.net/22644)
- [NNLM : A Neural Probabilistic Language Model](https://supkoon.tistory.com/16)
