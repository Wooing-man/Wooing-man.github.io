---
layout: single
title: "[Text Representation-5] Fasttext"
categories:	
    - nlp
tags:
    - ['딥러닝', '알고리즘', '자연어처리']

toc: true
toc_sticky : true

author_profile: false
sidebar:
    nav: "docs"

last_modified_at:
use_math: true
---

본 포스팅은 고려대학교 강필성 교수님의 [비정형데이터분석 강의](https://www.youtube.com/watch?v=UInnl60pzkA&list=PLetSlH8YjIfVzHuSXtG4jAC2zbEAErXWm)를 보고 정리한 것임을 미리 밝힙니다.

---

**Distributed Representation Models**

> - NNLM(2003) - Distributed Representation의 개념을 처음 도입한 Word Embedding계의 시조새
> - Word2Vec(2013) - 주변 단어들의 정보를 활용해서 원하는 단어의 Embedding을 학습 (CBOW vs Skip-gram)
> - Glove(2014) - 단어의 동시발생빈도를 고려해서 Embedding을 학습
> - <span style='background-color: #dcffe4'>Fasttext(2017) - 학습 과정에서 보지 못한 단어(OOV)들과 형태소 변화에 대응할 수 있도록 Sub-word 개념을 도입</span>

<br/>

# FastText

## 개요

Fasttext는 페이스북에서 개발한 것으로 Word2Vec은 단어를 쪼개질 수 없는 단위로 생각한 반면, FastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주한다. 한글의 경우 '그랬다면', '그랬으면', '그렇다면' 등등 비슷하지만 문맥상 다르게 쓰이는 단어들이 굉장히 많다. FastText는 Out-of_Vocabulary (OOV) words 문제를 풀고자 한다. OOV는 학습 과정에서 보지 못한 단어들에 대한 임베딩 벡터를 구할 수 없는 것을 뜻한다.

**요약**

- 다양한 형태소 변화에 대한 대응이 어려운 문제를 해결하고자 함(Morphology)
  - eat, eat**s**, eat**en**, eat**er**, eat**ing**
- 빈도가 적은 단어에 대한 정보를 많이 얻을 수 없다는 한계에 대해 어느 정도 정보를 포함하는 효과
- OOV 문제와 형태소 변화에 대응할 수 있도록 Sub-word 개념 도입
  1. 단어의 시작과 끝을 표기하기 위한 **bracket**추가 (띄어쓰기는 없음)
     - eating  ->  **<** eating **>**
  2. 가능한 단어 자체에 대한 모든 n-gram 추출
     - < eating > --> 3-grams
       - <ea \| eat \| ati \| tin \| ing \| ng>

<br/>

FastText는 학습할 때 Negative Sampling을 사용한 Skip-gram 방식을 차용한다

- I <u>am eating food</u> now

![](https://amitness.com/images/fasttext-toy-example.png){: .align-center}

- Center Word에 해당하는 단어는 N-gram 임베딩과 Word 임베딩을 모두 더한 임베딩을 <u>입력으로 사용</u>

![](https://amitness.com/images/fasttext-center-word-embedding.png){: .align-center}

- Context Word에 해당하는 단어들의 Output은 1에 가깝도록,
- Negative Samples에 해당하는 단어들의 Output은 0에 가깝도록 가중치를 업데이트

![](https://amitness.com/images/fasttext-negative-sampling-goal.png){: .align-center}

손실함수??

$$J_{t}(\theta) = log\sigma(u_o^Tv_c) + \sum_{i=1}^{k} E_{i \sim P(w)}[log\sigma (-u_i^Tv_c)]$$

자기 자신의 임베딩 벡터 + 단어의 각 n-gram 벡터??

$$score(w, c) = \sum_{g \in G_w} z_g^Tv_c$$





<br/>

<br/>

# 참고자료

---

- [고려대학교 강필성 교수 - 비정형데이터분석 강의](https://www.youtube.com/watch?v=bvSHJG-Fz3Y&list=PLetSlH8YjIfVzHuSXtG4jAC2zbEAErXWm&index=7)
- [딥러닝을 이용한 자연어처리 입문 - FastText](https://wikidocs.net/22883)
- [A Visual Guide to FastText Word Embeddings](https://amitness.com/2020/06/fasttext-embeddings/)
